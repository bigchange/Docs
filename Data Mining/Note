二、推荐系统入门

	内容：

	推荐系统工作原理
	社会化协同过滤工作原理
	如何找到相似物品
	曼哈顿距离
	欧几里得距离
	闵可夫斯基距离
	皮尔逊相关系数
	余弦相似度
	使用Python实现K最邻近算法
	图书漂流站（BookCrossing）数据集

1. 介绍协同过滤，基本的距离算法，包括曼哈顿距离、欧几里得距离、闵科夫斯基距离、皮尔森相关系数。使用Python实现一个基本的推荐算法。
	
	内容：

	推荐系统工作原理
	
	社会化协同过滤工作原理
	
	如何找到相似物品
	
	曼哈顿距离
	
	欧几里得距离
	
	闵可夫斯基距离
	
	皮尔逊相关系数
	
	余弦相似度
	
	使用Python实现K最邻近算法
	
	图书漂流站（BookCrossing）数据集
	
2. 协同过滤：这个方法是利用他人的喜好来进行推荐，也就是说，是大家一起产生的推荐。

    工作原理：如果要推荐一本书给你，我会在网站上查找一个和你类似的用户，然后将他喜欢的书籍推荐给你
	
	问题：曼哈顿距离和欧几里得距离在数据完整的情况下效果最好。如何处理缺失数据，这在研究领域仍是一个活跃的话题？
	
	皮尔逊相关系数：用于衡量两个变量之间的相关性（这里的两个变量指的是用户Clara和Robert），它的值在-1至1之间，1表示完全吻合，-1表示完全相悖。
	
	余弦相似度： 数据的稀疏性问题，《The Space Pioneers》中有6629个不同的单词，但英语语言中有超过100万个单词，这样一来非零值就很稀少了，也就不能计算两本书之间的距离。余弦相似度的计算中会略过这些非零值

3. 应该使用哪种相似度？

	如果数据存在“分数膨胀”问题，就使用皮尔逊相关系数。
	
	如果数据比较“密集”，变量之间基本都存在公有值，且这些距离数据是非常重要的，那就使用欧几里得或曼哈顿距离。
	
	如果数据是稀疏的，则使用余弦相似度。	
	
三、隐式评价和基于物品的过滤算法 
	
	内容：
	
	显式评价
	隐式评价
	哪种评价方式更准确？
	基于用户的协同过滤
	基于物品的协同过滤
	修正的余弦相似度
	Slope One算法
	Slope One的Python实现
	MovieLens数据

1. 显式评价和隐式评价

	显式评价指的是用户明确地给出对物品的评价; 所谓隐式评价，就是我们不让用户明确给出对物品的评价，而是通过观察他们的行为来获得偏好信息
	
	
	显式评价的问题： 问题1：人们很懒，不愿评价物品； 问题2：人们会撒谎，或存有偏见；问题3：人们不会更新他们的评论

2. 基于用户的协同过滤

	这种算法有两个弊端：
	
		扩展性 上文已经提到，随着用户数量的增加，其计算量也会增加。这种算法在只有几千个用户的情况下能够工作得很好，但达到一百万个用户时就会出现瓶颈。
		稀疏性  大多数推荐系统中，物品的数量要远大于用户的数量，因此用户仅仅对一小部分物品进行了评价，这就造成了数据的稀疏性。

3. 基于物品的协同过滤 : 一种算法可以计算出两件物品之间的相似度

	基于用户的协同过滤是通过计算用户之间的距离找出最相似的用户，并将他评价过的物品推荐给目标用户；而基于物品的协同过滤则是找出最相似的物品，再结合用户的评价来给出推荐结果。
	
	基于用户的协同过滤又称为内存型协同过滤，因为我们需要将所有的评价数据都保存在内存中来进行推荐。
    
	基于物品的协同过滤也称为基于模型的协同过滤，因为我们不需要保存所有的评价数据，而是通过构建一个物品相似度模型来做推荐。

4. 修正的余弦相似度: 我们使用余弦相似度来计算两个物品的距离。我们在第二章中提过“分数膨胀”现象，因此我们会从用户的评价中减去他所有评价的均值，这就是修正的余弦相似度。

5. Slope One算法: 一种比较流行的基于物品的协同过滤算法，它最大的优势是简单，因此易于实现


四、 分类

1. 十折交叉验证

	将数据集随机分割成十个等份，每次用9份数据做训练集，1份数据做测试集，如此迭代10次。

2. 留一法：在数据挖掘领域，N折交叉验证又称为留一法。

	留一法的另一个优点是：确定性。十折交叉验证中，即使是同一个人进行检验，如果两次使用了不同的分法，得到的结果也会有差异。相反，留一法得到的结果总是相同的
	
	最大的缺点是计算时间很长，留一法的另一个缺点是分层问题。
	
3. 混淆矩阵： 衡量分类器准确率的方式是使用以下公式：正确分类的记录数÷记录总数

	混淆矩阵的对角线（绿色字体）表示分类正确的人数，准确率：对角线之和 / 总数

	参看附图
	
4. Kappa指标： 可以用来评价分类器的效果比随机分类要好多少。

5. 近邻算法总有一个特殊的地方就是： 单个异常数据刚好里你需要分类的数据最近，导致结果不准确；所以解决办法就是取多个近邻比较

	kNN算法 （近邻算法的优化）：优化方法之一是考察这条新记录周围距离最近的k条记录，而不是只看一条，因此这种方法称为k近邻算法（kNN），每个近邻都有投票权，程序会将新记录判定为得票数最多的分类，如果两个分类的票数相等，就随机选取一个。
	
	但对于需要预测具体数值的情形，比如一个人对Funky Meters乐队的评分，我们可以计算k个近邻的距离加权平均值（1. 在计算平均值的时候，我希望距离越近的用户影响越大，因此可以对距离取倒数；2. 我们把所有的距离倒数除以距离倒数的和（0.2 + 0.1 + 0.067 = 0.367），从而得到评分的权重）。
	

五、 概率和朴素贝叶斯

	内容：

	朴素贝叶斯
	贝叶斯法则
	为什么我们需要贝叶斯法则？
	编写朴素贝叶斯分类器
	数值型数据

1. 贝叶斯算法则是一种主动学习算法，它会根据训练集构建起一个模型，并用这个模型来对新的记录进行分类，因此速度会快很多。近邻算法又称为被动学习算法。这种算法只是将训练集的数据保存起来，在收到测试数据时才会进行计算	

	所以说，贝叶斯算法的两个优点即：能够给出分类结果的置信度；以及它是一种主动学习算法。
	
    spark 贝叶斯中模型要求：特征为非负数，其中伯努利模型只要求存在0 或 1 特征值

2. 术语：P(h)表示事件h发生的概率，称为h的先验概率。P(h|d)称为后验概率，表示在观察了数据集d之后，h事件发生的概率是多少。

	很多时候我们会用到不止一个前提条件，比如判断一个人是否会购买Sencha绿茶时可以用到顾客所在地以及是否买过有机食品这两个条件。计算这样的概率时只需将各个条件概率相乘即可（前提是假设这两个条件之间是相互独立的）：
	
	P(买绿茶|88005 & 买有机食品) = P(88005|买绿茶)P(买有机食品|买绿茶)P(买绿茶) = 0.6 0.8 0.5 = 0.24
	
	P(┐买绿茶|88005 & 买有机食品) = P(88005|┐买绿茶)P(买有机食品|┐买绿茶)P(┐买绿茶) = 0.4 0.25 0.5 = 0.05
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	