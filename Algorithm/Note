1. PageRank
	(pageID, Linklist): 每个页面相邻页面的列表
	(pageID, rank): 每个页面当前的排序值
	迭代过程：
		（1）将每个页面的排序值初始化为1.0
		（2）在每次迭代中，对页面p，向其每个相邻页面（有直接连接的页面）发送一个值为rank(p) / numberNeighbors(p)的贡献值
		（3）将每个页面的排序值设为 0.15 + 0.85 * contributionReceived (contributionReceived：某个页面可能会受到多个贡献值，按照pageID分组累加之后再赋值即可 and why? 1 = 0.15 + 0.85)
		 最后两步收敛通常在10轮迭代左右
		 
	spark实现：
	val links = sc.objectFile[(String, Seq[String])]("links").partitionBy(new HashPartitioner(100)).persist()
	val ranks = links.mapValues(v => 1.0)
	
	for (i <- 0 until 10) {
		val contributionReceived = links.join(ranks).flatMap {
			case(pageID, (linksSeq, rank)) =>
				linksSeq.map(dest => (dest, rank / linksSeq.size))
		}
		
		ranks = contributionReceived.reduceByKey(_+_).mapValues(v => 0.15 + 0.85 * v)
	}
	
	ranks.saveAsTextFile("ranks")
	
	代码高效分析：
		（1）links rdd 每次迭代都有连接操作， 一开始进行分区操作，这样他就不需要进行网络数据混洗了，并可持续化到内存供后续迭代使用

		（2） 第一次ranks 使用mapValues() 而不是map 来保留父RDD的分区方式，这样第一次连接操作开销很少
		
		（3） 在reduceByKey() 后使用 mapValues() 因为 reduceByKey() 的结果已经是哈希分区了，这样一来下一次循环中再次与links进行连接操作就会更加高效
		
		（4） 为了最大化分区相关优化的潜在作用， 应该在无需改变原始数据键值的时候尽量使用mapValues() 或 flatMapValues() 
		
	影响分区方式的操作
		会为生成结果RDD设定好分区方式的操作有：cogroup()、groupWith、join、 leftOuterJoin、 rightOuterJoin、 groupByKey、 reduceByKey、 combineByKey、 partitionBy、sort、 mapValues()（如果父RDD有分区方式的话）、flatMapValues()（如果父RDD有分区方式的话）
		以及filter()（如果父RDD有分区方式的话），其他所有的操作结果都不会存在特定的分区方式。最后，对于二元操作，输出的数据分区方式取决于父RDD的分区方式。默认情况下，结果会采用哈希分区的方式，分区的数量和并行度一样，不过如果父RDD已经设置过了分区的方式的话，那么结果就会采用那种分区的方式，如果两个父RDD都设置过分区那么采用第一个父RDD的分区方式
		
	HashPartitioner，RangePartitioner，自定义分区方式
	基于域名的分区实现：
	class DomainNamePartitioner（numParts：Int） extends org.apache.spark.Partitioner {
		override def numPartitions: Int = numParts
		override def getPartition(key: Any): Int = {
			val domain = new Java.net.URL(key.toString).getHost()
			val code = (domain.hashCode % numPartitions)
			if(code < 0)
				code + numPartitions
			else
				code
		}
		// 用来让spark区分分区函数对象的java equals方法
		override def equals(other: Any): Boolean = other match {
			case dnp: DomainNamePartitioner =>
				dnp.numPartitions == numPartitions
			case _ => false
		}
	}
	
	自定义的累计器操作：
		必须是同时满足交换律和结合律（sum 和 max 满足）
		
	基于分区进行操作
		共享连接池和JSON解析器: mapPartitions()、 mapPartitionWithIndex()、 foreachPartition()
		
	与外部程序间的通道
		rdd.pipe()、 sc.addFile(path)
		
	执行器节点
		负责运行spark任务，并将结果返回给驱动器节点，通过自身的块管理器为用户程序要求缓存的RDD提供内存式存储，rdd是直接缓存在执行器的进程内的。
	
	spark应用执行流程
		通过spark-submit脚本提交spark应用
		脚本启动驱动器程序，调用用户定义的main方法
		驱动器程序与集群管理器通信，申请资源以启动执行器节点
		集群管理为驱动器程序启动执行器节点
		驱动器进程执行用户应用中的操作。根据定义的rdd装换和行动操作，驱动节点把工作以任务的形式发送到执行器进程
		任务在执行器程序中进行计算并保存结果
		main退出或sc.stop ,驱动程序会终止执行器进程，并且通过集群管理器释放资源
		
	JDBC/ODBC 服务器 、 Beeline
	
	Streaming
		nc/ncat/nmap 向套接字发送数据命令
		检查点机制，容错性提高， 一般每处理5-10个批次的数据保存一次，再恢复的时候回溯到上个检查点即可
		有状态和无状态转化操作
		有状态装换： 滑动窗口(窗口大小，滑动步长 - 都必须为批次间隔的整数倍，含义为：每隔滑动步长的时间，计算前窗口大小的批次数据)，updateStateByKey(),需要提前打开检查点来确保容错（ssc.checkpoint("hdfs://")）
		transform()

		reduceByKeyAndWindow()

		updateStateByKey 实例跟踪HTTP响应次数(键，事件列表) -> (键， 状态)
			def updataRunningSum(values:Seq[Long], state:Option[Long]) = { Some(state.getOrElse(0L) + values.size)}
			val respondseCodeDstream = dataRDD.map(log => (log.getRespondCode(),1L))
			val respondseCodeCountDstream = respondseCodeDstream.updateStateByKey(updataRunningSum _)
		
		存储数据到外部系统
			rdd.foreachRDD { rdd =>
				rdd.foreachPartition { partition =>
				 // 打开存储系统的连接（比如一个数据库的连接）
				 partition.foreach { item =>
					//使用连接把item 存到外部系统
				 }
				 // 关闭连接
				}
			}
		
		Apache Flume 推式接收器和拉式接收器
		
		配置不间断的运行
			第一步设置检查点机制
			第二步驱动器程序容错：
				def createStreamingContext = {
				  ...
				  // val sc = new SparkContext(conf)
				  val sparkConf = new SparkConf()
				  val ssc = new StreamingContext(sparkConf, Seconds(1))
				  ssc.checkpoint(checkpoint_dir)
				  
				}
				...
				val ssc = StreamingContext.getOrCreate(checkpoint_dir, createStreamingContext _)
		    
			第三步使用monit工具监视驱动器程序进程并重启（或者在提交驱动器程序的时候使用参数--supervise标记来让Spark重启失败的驱动器程序）
		
		性能考量
			批次和窗口大小 调整
			并行度
				增加接收器的数目通过union 合并为一个数据源
				将收到的数据显式的进行重新分区
				提高聚合度计算的并行度（reduceByKey 等操作中指定并行度）
			垃圾回收和内存使用
				配置参数 spark.executor.extraJavaOptions=-XX:+UseConcMarkSweepGC 打开java的并发标志-消除收集器
				以序列化的格式缓存
				控制缓存移除的策略（SPARK使用LRU缓存，可以通过spark.cleaner.ttl配置超出给定时间范围就显式的移除RDD）
				
2. 机器学习
	
	向量算术操作：scala 的 Breeze
	特征提取
		TF-IDF： 词频-逆文档频率
		缩放： 均值和std（StandardScaler）
		正规化：Normalizer 向量正规化为长度1
		Word2vec： 基于神经网络的文本特征化算法，可以将数据传给许多下游算法。该模型的大小等于你的词库中的单词数乘以向量的大小（向量大小默认为100），比较适合的词库大小约为100 000个词
		统计： mllib.stat.Statistics
		线性回归、 逻辑回归、 支持向量机、 朴素贝叶斯、 决策树和随机森林、 聚类、协同过滤
		降维
			主成分分析PCA
			奇异值分解
		模型评估
			BinaryClassificationMetrics 和 MulticlassMetric - 准确率、 召回率、 ROC曲线下面积
			缩放输入特征、 正确提取文本特征（NLTK 这样的外部库来提取词）、标上正确的标签等
		
		流水线API(ML包)
			垃圾分类实例
			case class LabelDoc(id:Long, text:String, label:Double)
			val doc = // 读取LabelDoc的RDD
			val sqlContext = new SQLContext(sc)
			import sqlContext._
			// 配置流水线的步骤：分词、词频统计、逻辑回归、每个步骤会输出SchemaRDD的一个列，并作为下一个步骤的输入列
			val tokenizer = new Tokenizer().setInputCol("text").setOutputCol("words")
			val tf  = new HashingTF().setNumFeatures(10000).setInputCol(tokenizer.getOutputCol).setOutputCol("features")
			val lr = new LogisticRegression() // 默认使用”features“作为输入列
			val pipe = new Pipeline().setStages(Array(tokenizer, tf, lr))
			
			// 使用流水线对训练文档进行拟合
			val model = pipe.fit(doc)
			// 也可以通过交叉验证对一批参数进行网格搜索，找到最佳模型
			val paramMaps = new ParamGridBuilder().addGrid(tf.numFeatures,Array(10000, 20000)).addGrid(lr.maxIter, Array(100, 200)).build()
			val eval = new BinaryClassificationEvaluator()
			val cv = new CrossValidator().setEstimator(lr).setEstimatorParamMaps(paramMaps).setEvaluator(eval)
			val bestModel = cv.fit(doc)
			
		